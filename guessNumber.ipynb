{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image, ImageOps\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import os\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import kagglehub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "idx1_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "loader = DataLoader(idx1_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "idx1_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "loaderTest = DataLoader(idx1_test, batch_size=64, shuffle=False)\n",
    "\n",
    "for X,y in loader:\n",
    "    print(X.shape)\n",
    "    print(y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: mps\n"
     ]
    }
   ],
   "source": [
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NueralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.network = nn.Sequential(nn.Linear(28*28, 512), \n",
    "                                     nn.ReLU(), \n",
    "                                     nn.Linear(512, 512),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(512,10))\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.network(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Base Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.315034  [   64/60000]\n",
      "loss: 2.286483  [ 6464/60000]\n",
      "loss: 2.239048  [12864/60000]\n",
      "loss: 2.193045  [19264/60000]\n",
      "loss: 2.144700  [25664/60000]\n",
      "loss: 2.087162  [32064/60000]\n",
      "loss: 2.023291  [38464/60000]\n",
      "loss: 1.974257  [44864/60000]\n",
      "loss: 1.857503  [51264/60000]\n",
      "loss: 1.887936  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.777145 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.888212  [   64/60000]\n",
      "loss: 1.689619  [ 6464/60000]\n",
      "loss: 1.598794  [12864/60000]\n",
      "loss: 1.508066  [19264/60000]\n",
      "loss: 1.500436  [25664/60000]\n",
      "loss: 1.262134  [32064/60000]\n",
      "loss: 1.219414  [38464/60000]\n",
      "loss: 1.051600  [44864/60000]\n",
      "loss: 0.970804  [51264/60000]\n",
      "loss: 0.958521  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.966234 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.036318  [   64/60000]\n",
      "loss: 0.971765  [ 6464/60000]\n",
      "loss: 0.980676  [12864/60000]\n",
      "loss: 0.833992  [19264/60000]\n",
      "loss: 0.856325  [25664/60000]\n",
      "loss: 0.733275  [32064/60000]\n",
      "loss: 0.720725  [38464/60000]\n",
      "loss: 0.654422  [44864/60000]\n",
      "loss: 0.671260  [51264/60000]\n",
      "loss: 0.709302  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.636281 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.785530  [   64/60000]\n",
      "loss: 0.509635  [ 6464/60000]\n",
      "loss: 0.535063  [12864/60000]\n",
      "loss: 0.497993  [19264/60000]\n",
      "loss: 0.584981  [25664/60000]\n",
      "loss: 0.661422  [32064/60000]\n",
      "loss: 0.564045  [38464/60000]\n",
      "loss: 0.499105  [44864/60000]\n",
      "loss: 0.385309  [51264/60000]\n",
      "loss: 0.682597  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.504038 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.577556  [   64/60000]\n",
      "loss: 0.457414  [ 6464/60000]\n",
      "loss: 0.413410  [12864/60000]\n",
      "loss: 0.521199  [19264/60000]\n",
      "loss: 0.618979  [25664/60000]\n",
      "loss: 0.460521  [32064/60000]\n",
      "loss: 0.536271  [38464/60000]\n",
      "loss: 0.471263  [44864/60000]\n",
      "loss: 0.420651  [51264/60000]\n",
      "loss: 0.493760  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.435305 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = NueralNetwork().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "epochs = 5 \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loader, model, loss_fn, optimizer)\n",
    "    test(loaderTest, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "- `NueralNetwork` outputs raw logits, which align correctly with `nn.CrossEntropyLoss`.\n",
    "- The use of `SGD` with a learning rate of `1e-3` leads to stable training and good accuracy (~88.9%).\n",
    "- This setup is mathematically correct and performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configurations and Results\n",
    "\n",
    "### **2. Changed Optimizer Type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.319017  [   64/60000]\n",
      "loss: 2.240031  [ 6464/60000]\n",
      "loss: 2.219429  [12864/60000]\n",
      "loss: 2.202428  [19264/60000]\n",
      "loss: 2.160276  [25664/60000]\n",
      "loss: 2.112474  [32064/60000]\n",
      "loss: 2.052041  [38464/60000]\n",
      "loss: 1.947716  [44864/60000]\n",
      "loss: 1.901538  [51264/60000]\n",
      "loss: 1.777352  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.8%, Avg loss: 1.739431 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.791808  [   64/60000]\n",
      "loss: 1.715565  [ 6464/60000]\n",
      "loss: 1.701730  [12864/60000]\n",
      "loss: 1.471373  [19264/60000]\n",
      "loss: 1.415198  [25664/60000]\n",
      "loss: 1.389085  [32064/60000]\n",
      "loss: 1.271994  [38464/60000]\n",
      "loss: 1.175642  [44864/60000]\n",
      "loss: 1.142135  [51264/60000]\n",
      "loss: 1.280625  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.911690 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.015160  [   64/60000]\n",
      "loss: 1.057798  [ 6464/60000]\n",
      "loss: 0.920756  [12864/60000]\n",
      "loss: 1.131007  [19264/60000]\n",
      "loss: 0.929866  [25664/60000]\n",
      "loss: 1.012021  [32064/60000]\n",
      "loss: 0.821520  [38464/60000]\n",
      "loss: 0.875581  [44864/60000]\n",
      "loss: 0.716226  [51264/60000]\n",
      "loss: 0.609165  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.5%, Avg loss: 0.591750 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.748611  [   64/60000]\n",
      "loss: 0.996723  [ 6464/60000]\n",
      "loss: 0.804525  [12864/60000]\n",
      "loss: 0.773654  [19264/60000]\n",
      "loss: 0.692701  [25664/60000]\n",
      "loss: 0.654242  [32064/60000]\n",
      "loss: 0.583153  [38464/60000]\n",
      "loss: 0.717595  [44864/60000]\n",
      "loss: 0.598224  [51264/60000]\n",
      "loss: 0.608538  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.3%, Avg loss: 0.468550 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.673865  [   64/60000]\n",
      "loss: 0.616463  [ 6464/60000]\n",
      "loss: 0.433814  [12864/60000]\n",
      "loss: 0.603115  [19264/60000]\n",
      "loss: 0.691583  [25664/60000]\n",
      "loss: 0.576855  [32064/60000]\n",
      "loss: 0.611524  [38464/60000]\n",
      "loss: 0.616149  [44864/60000]\n",
      "loss: 0.612372  [51264/60000]\n",
      "loss: 0.521293  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.3%, Avg loss: 0.403948 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adadelta(model.parameters(), lr = 1e-3)\n",
    "\n",
    "epochs = 5 \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loader, model, loss_fn, optimizer)\n",
    "    test(loaderTest, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "- `Net()` outputs log probabilities using `F.log_softmax`.\n",
    "- `nn.CrossEntropyLoss` expects raw logits, so this configuration applies `log_softmax` twice: `log_softmax(log(softmax(...)))`.  \n",
    "- This mismatch leads to suboptimal gradient computation but still achieves decent accuracy (~89.5%) due to the robustness of neural networks. However, it is not mathematically sound and may not generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Changed Loss Function (`loss_fn`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305311  [   64/60000]\n",
      "loss: 2.214709  [ 6464/60000]\n",
      "loss: 1.968969  [12864/60000]\n",
      "loss: 1.829661  [19264/60000]\n",
      "loss: 1.414545  [25664/60000]\n",
      "loss: 1.171876  [32064/60000]\n",
      "loss: 1.018690  [38464/60000]\n",
      "loss: 1.005191  [44864/60000]\n",
      "loss: 0.940643  [51264/60000]\n",
      "loss: 0.641867  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.509963 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.678478  [   64/60000]\n",
      "loss: 0.685668  [ 6464/60000]\n",
      "loss: 0.541660  [12864/60000]\n",
      "loss: 0.357854  [19264/60000]\n",
      "loss: 0.521981  [25664/60000]\n",
      "loss: 0.525979  [32064/60000]\n",
      "loss: 0.607698  [38464/60000]\n",
      "loss: 0.583878  [44864/60000]\n",
      "loss: 0.526609  [51264/60000]\n",
      "loss: 0.617558  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.9%, Avg loss: 0.336854 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.486879  [   64/60000]\n",
      "loss: 0.445529  [ 6464/60000]\n",
      "loss: 0.268462  [12864/60000]\n",
      "loss: 0.704240  [19264/60000]\n",
      "loss: 0.494975  [25664/60000]\n",
      "loss: 0.429579  [32064/60000]\n",
      "loss: 0.240701  [38464/60000]\n",
      "loss: 0.344638  [44864/60000]\n",
      "loss: 0.246383  [51264/60000]\n",
      "loss: 0.360047  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.8%, Avg loss: 0.280394 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.410048  [   64/60000]\n",
      "loss: 0.695118  [ 6464/60000]\n",
      "loss: 0.435307  [12864/60000]\n",
      "loss: 0.498319  [19264/60000]\n",
      "loss: 0.548674  [25664/60000]\n",
      "loss: 0.272176  [32064/60000]\n",
      "loss: 0.321966  [38464/60000]\n",
      "loss: 0.309241  [44864/60000]\n",
      "loss: 0.523155  [51264/60000]\n",
      "loss: 0.315412  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.8%, Avg loss: 0.248966 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.315765  [   64/60000]\n",
      "loss: 0.383385  [ 6464/60000]\n",
      "loss: 0.358717  [12864/60000]\n",
      "loss: 0.328836  [19264/60000]\n",
      "loss: 0.399611  [25664/60000]\n",
      "loss: 0.426870  [32064/60000]\n",
      "loss: 0.263697  [38464/60000]\n",
      "loss: 0.274209  [44864/60000]\n",
      "loss: 0.459997  [51264/60000]\n",
      "loss: 0.286785  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.220030 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loader, model, loss_fn, optimizer)\n",
    "    test(loaderTest, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "- Using `nn.NLLLoss` matches `Net()`'s output of log probabilities (`F.log_softmax`), ensuring proper gradient computation.\n",
    "- This alignment improves training dynamics, leading to better accuracy (~93.4%).\n",
    "- Correct loss function selection is critical for effective training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Changed Loss Function and Optimizer Learning Rate to 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300282  [   64/60000]\n",
      "loss:     nan  [ 6464/60000]\n",
      "loss:     nan  [12864/60000]\n",
      "loss:     nan  [19264/60000]\n",
      "loss:     nan  [25664/60000]\n",
      "loss:     nan  [32064/60000]\n",
      "loss:     nan  [38464/60000]\n",
      "loss:     nan  [44864/60000]\n",
      "loss:     nan  [51264/60000]\n",
      "loss:     nan  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss:      nan \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss:     nan  [   64/60000]\n",
      "loss:     nan  [ 6464/60000]\n",
      "loss:     nan  [12864/60000]\n",
      "loss:     nan  [19264/60000]\n",
      "loss:     nan  [25664/60000]\n",
      "loss:     nan  [32064/60000]\n",
      "loss:     nan  [38464/60000]\n",
      "loss:     nan  [44864/60000]\n",
      "loss:     nan  [51264/60000]\n",
      "loss:     nan  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss:      nan \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss:     nan  [   64/60000]\n",
      "loss:     nan  [ 6464/60000]\n",
      "loss:     nan  [12864/60000]\n",
      "loss:     nan  [19264/60000]\n",
      "loss:     nan  [25664/60000]\n",
      "loss:     nan  [32064/60000]\n",
      "loss:     nan  [38464/60000]\n",
      "loss:     nan  [44864/60000]\n",
      "loss:     nan  [51264/60000]\n",
      "loss:     nan  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss:      nan \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss:     nan  [   64/60000]\n",
      "loss:     nan  [ 6464/60000]\n",
      "loss:     nan  [12864/60000]\n",
      "loss:     nan  [19264/60000]\n",
      "loss:     nan  [25664/60000]\n",
      "loss:     nan  [32064/60000]\n",
      "loss:     nan  [38464/60000]\n",
      "loss:     nan  [44864/60000]\n",
      "loss:     nan  [51264/60000]\n",
      "loss:     nan  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss:      nan \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss:     nan  [   64/60000]\n",
      "loss:     nan  [ 6464/60000]\n",
      "loss:     nan  [12864/60000]\n",
      "loss:     nan  [19264/60000]\n",
      "loss:     nan  [25664/60000]\n",
      "loss:     nan  [32064/60000]\n",
      "loss:     nan  [38464/60000]\n",
      "loss:     nan  [44864/60000]\n",
      "loss:     nan  [51264/60000]\n",
      "loss:     nan  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.8%, Avg loss:      nan \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Changed loss_f and optimizer to 1\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer= torch.optim.SGD(model.parameters(), lr = 1)\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loader, model, loss_fn, optimizer) \n",
    "    test(loaderTest, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This is Wrong:\n",
    "1. **Loss Function (`nn.NLLLoss`) is Matched Correctly:**\n",
    "   - The `Net()` model outputs log probabilities using `F.log_softmax`, which is the correct input format for `nn.NLLLoss`.\n",
    "   - Therefore, the loss function is **not the source of the issue**.\n",
    "\n",
    "2. **Learning Rate (`lr=1`) is Too High:**\n",
    "   - A learning rate of `1` for `SGD` causes gradient updates to overshoot the optimal weights, preventing effective convergence.\n",
    "   - The stagnant accuracy (~11%) and consistently high loss values across epochs indicate that the model is not learning.\n",
    "\n",
    "3. **Lack of Convergence:**\n",
    "   - The combination of a high learning rate and `SGD` results in excessively large weight updates, destabilizing the training process.\n",
    "   - This prevents the model from optimizing properly, leading to poor performance.\n",
    "\n",
    "4. **High Loss Values Persist:**\n",
    "   - With a high learning rate, gradients likely explode or oscillate, making it difficult for the model to stabilize and reduce the loss.\n",
    "   - The model essentially guesses randomly, reflected by the accuracy (~11%, similar to random guessing for a 10-class problem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Changed Optimizer Type and Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.327518  [   64/60000]\n",
      "loss: 2.268046  [ 6464/60000]\n",
      "loss: 2.247958  [12864/60000]\n",
      "loss: 2.208548  [19264/60000]\n",
      "loss: 2.165229  [25664/60000]\n",
      "loss: 2.094087  [32064/60000]\n",
      "loss: 2.024014  [38464/60000]\n",
      "loss: 1.990599  [44864/60000]\n",
      "loss: 1.824877  [51264/60000]\n",
      "loss: 1.749631  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 1.619667 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.629912  [   64/60000]\n",
      "loss: 1.588315  [ 6464/60000]\n",
      "loss: 1.454926  [12864/60000]\n",
      "loss: 1.373632  [19264/60000]\n",
      "loss: 1.249967  [25664/60000]\n",
      "loss: 1.194409  [32064/60000]\n",
      "loss: 1.212227  [38464/60000]\n",
      "loss: 1.073718  [44864/60000]\n",
      "loss: 1.160255  [51264/60000]\n",
      "loss: 1.152254  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.9%, Avg loss: 0.792541 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.855385  [   64/60000]\n",
      "loss: 0.902549  [ 6464/60000]\n",
      "loss: 0.860673  [12864/60000]\n",
      "loss: 0.833669  [19264/60000]\n",
      "loss: 0.943965  [25664/60000]\n",
      "loss: 0.738686  [32064/60000]\n",
      "loss: 0.940035  [38464/60000]\n",
      "loss: 0.711550  [44864/60000]\n",
      "loss: 0.778744  [51264/60000]\n",
      "loss: 0.747836  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.540357 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.640081  [   64/60000]\n",
      "loss: 0.688190  [ 6464/60000]\n",
      "loss: 0.710737  [12864/60000]\n",
      "loss: 0.683514  [19264/60000]\n",
      "loss: 0.681833  [25664/60000]\n",
      "loss: 0.599701  [32064/60000]\n",
      "loss: 0.833762  [38464/60000]\n",
      "loss: 0.570096  [44864/60000]\n",
      "loss: 0.669781  [51264/60000]\n",
      "loss: 0.710626  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.9%, Avg loss: 0.438046 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.705072  [   64/60000]\n",
      "loss: 0.656316  [ 6464/60000]\n",
      "loss: 0.445057  [12864/60000]\n",
      "loss: 0.598303  [19264/60000]\n",
      "loss: 0.458001  [25664/60000]\n",
      "loss: 0.481396  [32064/60000]\n",
      "loss: 0.510554  [38464/60000]\n",
      "loss: 0.694228  [44864/60000]\n",
      "loss: 0.402585  [51264/60000]\n",
      "loss: 0.545468  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.9%, Avg loss: 0.385613 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr = 1e-3)\n",
    "\n",
    "epochs = 5 \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loader, model, loss_fn, optimizer)\n",
    "    test(loaderTest, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Achieved 90.4% Accuracy\n",
    "\n",
    "1. **Correct Loss Function (`nn.NLLLoss`):**\n",
    "   - The `Net()` model outputs log probabilities using `F.log_softmax`, which align perfectly with `nn.NLLLoss`.\n",
    "   - This ensures proper gradient computation and stable training, enabling the model to optimize effectively.\n",
    "\n",
    "2. **Adaptive Optimizer (`Adadelta`):**\n",
    "   - `Adadelta` dynamically adjusts the learning rate during training, making it robust to small initial learning rates like `1e-3`.\n",
    "   - Its adaptability ensures stable convergence even with challenging hyperparameter settings.\n",
    "\n",
    "3. **Balanced Training Dynamics:**\n",
    "   - The combination of a matched loss function and a robust optimizer allowed the model to learn efficiently.\n",
    "   - Training for five epochs provided sufficient time for the model to achieve **90.4% accuracy** on the MNIST test dataset.\n",
    "\n",
    "4. **Conclusion:**\n",
    "   - This setup highlights the importance of aligning the loss function with the model's output and selecting an appropriate optimizer for effective learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Changed Optimizer to 1 and Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.295103  [   64/60000]\n",
      "loss: 0.300671  [ 6464/60000]\n",
      "loss: 0.345443  [12864/60000]\n",
      "loss: 0.215815  [19264/60000]\n",
      "loss: 0.084297  [25664/60000]\n",
      "loss: 0.364286  [32064/60000]\n",
      "loss: 0.036912  [38464/60000]\n",
      "loss: 0.058033  [44864/60000]\n",
      "loss: 0.162910  [51264/60000]\n",
      "loss: 0.120553  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.045810 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.010138  [   64/60000]\n",
      "loss: 0.055568  [ 6464/60000]\n",
      "loss: 0.083560  [12864/60000]\n",
      "loss: 0.097783  [19264/60000]\n",
      "loss: 0.146802  [25664/60000]\n",
      "loss: 0.230263  [32064/60000]\n",
      "loss: 0.065327  [38464/60000]\n",
      "loss: 0.103895  [44864/60000]\n",
      "loss: 0.143842  [51264/60000]\n",
      "loss: 0.082730  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.036745 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.017039  [   64/60000]\n",
      "loss: 0.011097  [ 6464/60000]\n",
      "loss: 0.060552  [12864/60000]\n",
      "loss: 0.016573  [19264/60000]\n",
      "loss: 0.017958  [25664/60000]\n",
      "loss: 0.028073  [32064/60000]\n",
      "loss: 0.003336  [38464/60000]\n",
      "loss: 0.010043  [44864/60000]\n",
      "loss: 0.000479  [51264/60000]\n",
      "loss: 0.064757  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.030502 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.049805  [   64/60000]\n",
      "loss: 0.049711  [ 6464/60000]\n",
      "loss: 0.024216  [12864/60000]\n",
      "loss: 0.077916  [19264/60000]\n",
      "loss: 0.010904  [25664/60000]\n",
      "loss: 0.079932  [32064/60000]\n",
      "loss: 0.005544  [38464/60000]\n",
      "loss: 0.163320  [44864/60000]\n",
      "loss: 0.006525  [51264/60000]\n",
      "loss: 0.077506  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.028403 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.016264  [   64/60000]\n",
      "loss: 0.018262  [ 6464/60000]\n",
      "loss: 0.001197  [12864/60000]\n",
      "loss: 0.020006  [19264/60000]\n",
      "loss: 0.138489  [25664/60000]\n",
      "loss: 0.345029  [32064/60000]\n",
      "loss: 0.075535  [38464/60000]\n",
      "loss: 0.029128  [44864/60000]\n",
      "loss: 0.019009  [51264/60000]\n",
      "loss: 0.015127  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.028689 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "    \n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr = 1)\n",
    "\n",
    "epochs = 5 \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loader, model, loss_fn, optimizer)\n",
    "    test(loaderTest, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "- Proper alignment between `nn.NLLLoss` and `Net()`'s log probability output ensures correct gradient computation.\n",
    "- Using `SGD` with a high learning rate of `1` accelerates convergence.\n",
    "- This setup is mathematically sound and performs well, achieving consistent high accuracy (~93.4%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.291305  [   64/60000]\n",
      "loss: 1.413808  [ 6464/60000]\n",
      "loss: 1.000015  [12864/60000]\n",
      "loss: 0.667317  [19264/60000]\n",
      "loss: 0.648090  [25664/60000]\n",
      "loss: 0.371900  [32064/60000]\n",
      "loss: 0.803730  [38464/60000]\n",
      "loss: 0.762061  [44864/60000]\n",
      "loss: 2.310365  [51264/60000]\n",
      "loss: 2.312693  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.303194 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.291839  [   64/60000]\n",
      "loss: 2.311959  [ 6464/60000]\n",
      "loss: 2.273559  [12864/60000]\n",
      "loss: 2.294441  [19264/60000]\n",
      "loss: 2.283589  [25664/60000]\n",
      "loss: 2.304290  [32064/60000]\n",
      "loss: 2.318990  [38464/60000]\n",
      "loss: 2.281663  [44864/60000]\n",
      "loss: 2.314061  [51264/60000]\n",
      "loss: 2.293481  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.303098 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.303495  [   64/60000]\n",
      "loss: 2.311114  [ 6464/60000]\n",
      "loss: 2.300283  [12864/60000]\n",
      "loss: 2.299111  [19264/60000]\n",
      "loss: 2.321725  [25664/60000]\n",
      "loss: 2.300591  [32064/60000]\n",
      "loss: 2.297919  [38464/60000]\n",
      "loss: 2.312478  [44864/60000]\n",
      "loss: 2.327009  [51264/60000]\n",
      "loss: 2.312166  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.7%, Avg loss: 2.308515 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.321424  [   64/60000]\n",
      "loss: 2.305521  [ 6464/60000]\n",
      "loss: 2.312172  [12864/60000]\n",
      "loss: 2.321410  [19264/60000]\n",
      "loss: 2.303493  [25664/60000]\n",
      "loss: 2.324118  [32064/60000]\n",
      "loss: 2.306866  [38464/60000]\n",
      "loss: 2.300491  [44864/60000]\n",
      "loss: 2.321572  [51264/60000]\n",
      "loss: 2.317198  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.301442 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.300050  [   64/60000]\n",
      "loss: 2.288707  [ 6464/60000]\n",
      "loss: 2.304353  [12864/60000]\n",
      "loss: 2.317333  [19264/60000]\n",
      "loss: 2.302533  [25664/60000]\n",
      "loss: 2.310591  [32064/60000]\n",
      "loss: 2.311329  [38464/60000]\n",
      "loss: 2.313430  [44864/60000]\n",
      "loss: 2.299175  [51264/60000]\n",
      "loss: 2.305772  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 11.3%, Avg loss: 2.305177 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Changed optimizer to 1,  and loss_fn\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1)\n",
    "\n",
    "epochs = 5 \n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(loader, model, loss_fn, optimizer)\n",
    "    test(loaderTest, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why This Achieved Terrible Accuracy (11.3%)\n",
    "\n",
    "1. **Learning Rate Too High (`lr=1`):**  \n",
    "   - Using `SGD` with a high learning rate (`lr=1`) caused the model's weight updates to overshoot the optimal values. This prevented convergence and led to consistently poor accuracy (~11.3%, akin to random guessing).\n",
    "\n",
    "2. **Correct Loss Function (`nn.NLLLoss`):**  \n",
    "   - The `Net()` model outputs log probabilities, and `nn.NLLLoss` is the correct choice for this output format. However, the high learning rate undermined the training process, rendering the correct loss function ineffective.\n",
    "\n",
    "3. **Training Instability:**  \n",
    "   - The combination of `SGD` and an excessive learning rate led to large gradient updates, destabilizing training and preventing the model from reducing loss or improving accuracy.\n",
    "\n",
    "### Key Fix:  \n",
    "Reducing the learning rate (e.g., `lr=0.01` or `lr=0.1`) would allow the model to converge effectively and achieve much better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x31cbec450>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = enumerate(loaderTest)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "print(examples)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(example_data[i][0], cmap=\"gray\", interpolation='none')\n",
    "    plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ConvolutionalNueralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.conv1 = nn.Conv2d(in_channels = 1, out_channels=16, kernel_size=3, padding = 1, stride = 1)\n",
    "#         self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding = 1, stride = 1)\n",
    "#         self.conv2_drop = nn.Dropout2d()\n",
    "#         self.layer1 = nn.Linear(32*14*14, 128)\n",
    "#         self.layer2 = nn.Linear(128, 10)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = nn.ReLU(nn.MaxPool2d(self.conv1, 2))\n",
    "#         x = nn.ReLU(nn.MaxPool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "#         x = x.view(-1, 320)\n",
    "#         x = nn.ReLU(self.layer1(x))\n",
    "#         x = nn.Dropout(x, training = self.training)\n",
    "#         x = self.layer2(x)\n",
    "\n",
    "\n",
    "#         return nn.LogSoftmax(x)\n",
    "\n",
    "#         # # x = self.conv1(x)\n",
    "#         # # x = self.relu(x)\n",
    "#         # # x = self.conv2(x)\n",
    "#         # # x = self.relu(x)\n",
    "#         # # x = nn.MaxPool2d(x, 2)\n",
    "#         # # x = self.flatten(x)\n",
    "#         # # x = self.layer1(x)\n",
    "#         # # x = self.relu(x)\n",
    "#         # # x = self.layer2(x)\n",
    "#         # return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
